{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b50ca3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cornac\n",
    "import pandas as pd\n",
    "\n",
    "from cornac.data.text import TextModality\n",
    "from cornac.eval_methods import RatioSplit\n",
    "from cornac.models import BPR, PMF\n",
    "from cornac.models import VAECF\n",
    "from cornac.metrics import Precision, Recall, RMSE, MAE\n",
    "from cornac.data import Reader\n",
    "from cornac.eval_methods import RatioSplit\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445cd13c",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "27cf951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('.surprise_data\\\\ml-100k\\\\ml-100k\\\\u.data', sep='\\t', header=None, names=['user_id', 'item_id', 'rating', 'timestamp'])\n",
    "\n",
    "items = pd.read_csv('.surprise_data\\\\ml-100k\\\\ml-100k\\\\u.item', sep='|', header=None, encoding='latin1',\n",
    "            names=['item_id', 'movie_title', 'release_date', 'video_release_date', 'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western'])\n",
    "\n",
    "users = pd.read_csv('.surprise_data\\\\ml-100k\\\\ml-100k\\\\u.user', sep='|', header=None, encoding='latin1',\n",
    "            names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])\n",
    "#First Digit of the US zip code represents the a region group            \n",
    "users['zipcode_reduced'] = users['zip_code'].str[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7768be5a",
   "metadata": {},
   "source": [
    "In order to include items and user features, we need to create what's called a \"modality\" in Cornac.\n",
    "\n",
    "The modality uses a bag-of-words representation of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "6501a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The movie data is on the one-hot encoding format, therefore we need to concatenate it into a single column, and create a modality\n",
    "\n",
    "genre_cols = ['Action', 'Adventure', 'Animation', 'Childrens', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "\n",
    "def genres_to_text(row):\n",
    "    return ' '.join(genre for genre in genre_cols if row[genre] == 1)\n",
    "\n",
    "items['genre_concat']=items.apply(genres_to_text, axis=1)\n",
    "\n",
    "item_modality = TextModality(corpus=items['genre_concat'], ids=items.item_id)\n",
    "\n",
    "# The user modality is the occupation in this case\n",
    "user_modality = TextModality(corpus=users.occupation, ids=users.user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d6cad6",
   "metadata": {},
   "source": [
    "Definiting the dataset, Cornac offers a nice utility that handles the split between train and test.\n",
    "\n",
    "Because the data has an explicity rating, the rating_threshold is used for calculating some of the metrics such as Precision_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "22ff09aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating_threshold = 4.0\n",
      "exclude_unknowns = False\n",
      "---\n",
      "Training data:\n",
      "Number of users = 943\n",
      "Number of items = 1656\n",
      "Number of ratings = 80000\n",
      "Max rating = 5.0\n",
      "Min rating = 1.0\n",
      "Global mean = 3.5\n",
      "---\n",
      "Test data:\n",
      "Number of users = 943\n",
      "Number of items = 1682\n",
      "Number of ratings = 20000\n",
      "Number of unknown users = 0\n",
      "Number of unknown items = 26\n",
      "---\n",
      "Total users = 943\n",
      "Total items = 1682\n"
     ]
    }
   ],
   "source": [
    "reader = Reader()\n",
    "data = ratings[['user_id', 'item_id', 'rating']].values.tolist()\n",
    "\n",
    "rs = RatioSplit(\n",
    "    data=data,\n",
    "    test_size=0.2,\n",
    "    rating_threshold=4, \n",
    "    user_text=user_modality,\n",
    "    item_text=item_modality,\n",
    "    exclude_unknowns=False,\n",
    "    verbose=True,\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30a82df",
   "metadata": {},
   "source": [
    "## Model training and evaluation\n",
    "\n",
    "I am testing  different models\n",
    "\n",
    "BPR - Bayesian Personalized Ranking.\n",
    "\n",
    "PMF - Probabilistic Matrix Factorization.\n",
    "\n",
    "VAECF - Variational Autoencoder for Collaborative Filtering. Considers the user and item features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "d108583e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[BPR] Training started!\n",
      "\n",
      "[BPR] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rating: 100%|██████████| 20000/20000 [00:00<00:00, 171614.26it/s]\n",
      "Ranking: 100%|██████████| 942/942 [00:03<00:00, 260.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PMF] Training started!\n",
      "\n",
      "[PMF] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rating: 100%|██████████| 20000/20000 [00:00<00:00, 167706.42it/s]\n",
      "Ranking: 100%|██████████| 942/942 [00:00<00:00, 6101.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[VAECF] Training started!\n",
      "\n",
      "[VAECF] Evaluation started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rating: 100%|██████████| 20000/20000 [00:03<00:00, 5607.65it/s]\n",
      "Ranking: 100%|██████████| 942/942 [00:00<00:00, 2827.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST:\n",
      "...\n",
      "      |    MAE |   RMSE | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "----- + ------ + ------ + ------------ + --------- + --------- + --------\n",
      "BPR   | 2.0144 | 2.2268 |       0.1129 |    0.1199 |    1.3421 |   3.8465\n",
      "PMF   | 0.7538 | 0.9143 |       0.0813 |    0.0639 |    2.1912 |   0.3678\n",
      "VAECF | 2.5754 | 2.7651 |       0.1533 |    0.1714 |    8.8306 |   3.9634\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "models = [BPR(k=10, max_iter=200, learning_rate=0.001, lambda_reg=0.01, seed=123),\n",
    "            PMF(k=10, max_iter=100, learning_rate=0.001, lambda_reg=0.001, seed=123),\n",
    "            VAECF(k=10, use_gpu=False)]\n",
    "\n",
    "# Define metrics to evaluate the models\n",
    "metrics = [RMSE(), MAE(), Precision(k=10), Recall(k=10)]\n",
    "\n",
    "# Put it together in an experiment, voilà!\n",
    "cornac.Experiment(eval_method=rs, models=models, metrics=metrics, user_based=True).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8430e57",
   "metadata": {},
   "source": [
    "VAECF offered the best results, followed by BPR and PMF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4a6e2b",
   "metadata": {},
   "source": [
    "## Grid search to improve BPR and VAECF model\n",
    "\n",
    "- using precision@10 to optimize - I noticed that the precision@10 can vary drastically depending on the test, validation size, so keeping it at 20% to be comparable to the lightFM model split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "cdf7e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the split, adding validation set\n",
    "rs = RatioSplit(\n",
    "    data=data,\n",
    "    test_size=0.2,\n",
    "    val_size=0.2,\n",
    "    rating_threshold=4.0, \n",
    "    user_text=user_modality,\n",
    "    item_text=item_modality,\n",
    "    exclude_unknowns=True,\n",
    "    verbose=False,\n",
    "    seed=123\n",
    ")\n",
    "\n",
    "#rs = RatioSplit(data=data, test_size=0.1, val_size=0.1, rating_threshold=4.0, seed=123)\n",
    "\n",
    "# Instantiate the models\n",
    "bpr = BPR(k=10, max_iter=200, learning_rate=0.001, lambda_reg=0.01, seed=123)\n",
    "vaecf = VAECF(k=10, learning_rate=0.001, seed=123)\n",
    "\n",
    "\n",
    "# Defining the Grid Search parameters and metric to optimize, Precision@10\n",
    "gs_bpr = GridSearch(\n",
    "    model=bpr,\n",
    "    space=[\n",
    "        Discrete(name=\"k\", values=[5, 10]),\n",
    "        Discrete(name=\"learning_rate\", values=[0.001, 0.05])\n",
    "    ],\n",
    "    metric=Precision(k=10),\n",
    "    eval_method=rs,\n",
    ")\n",
    "\n",
    "# Grid Search\n",
    "gs_vaecf = GridSearch(\n",
    "    model=vaecf,\n",
    "    space=[\n",
    "        Discrete(name=\"k\", values=[5, 10]),\n",
    "        Discrete(name=\"learning_rate\", values=[0.001, 0.05]),\n",
    "        Discrete(name=\"autoencoder_structure\", values=[[20],[40]])        \n",
    "    ],\n",
    "    metric=Precision(k=10),\n",
    "    eval_method=rs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "8df8ed82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VALIDATION:\n",
      "...\n",
      "                 |    MAE |   RMSE | Precision@10 | Recall@10 | Time (s)\n",
      "---------------- + ------ + ------ + ------------ + --------- + --------\n",
      "GridSearch_VAECF | 2.5832 | 2.7713 |       0.1407 |    0.1693 |   4.5174\n",
      "GridSearch_BPR   | 1.5082 | 1.7492 |       0.1280 |    0.1503 |   3.7652\n",
      "\n",
      "TEST:\n",
      "...\n",
      "                 |    MAE |   RMSE | Precision@10 | Recall@10 | Train (s) | Test (s)\n",
      "---------------- + ------ + ------ + ------------ + --------- + --------- + --------\n",
      "GridSearch_VAECF | 2.5763 | 2.7657 |       0.1683 |    0.1874 |  102.0480 |   4.6161\n",
      "GridSearch_BPR   | 1.5013 | 1.7427 |       0.1467 |    0.1605 |   18.4331 |   3.7826\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment\n",
    "cornac.Experiment(eval_method=rs, models=[gs_vaecf, gs_bpr], metrics=[RMSE(), MAE(), Precision(k=10), Recall(k=10)], user_based=True).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "1711c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BRP best parameters for Precision@10\n",
      "{'k': 10, 'learning_rate': 0.05}\n",
      "VAECF best parameters for Precision@10\n",
      "{'autoencoder_structure': [40], 'k': 5, 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "# Obtain the best params\n",
    "print('BRP best parameters for Precision@10')\n",
    "print(gs_bpr.best_params)\n",
    "print('VAECF best parameters for Precision@10')\n",
    "print(gs_vaecf.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb03573e",
   "metadata": {},
   "source": [
    "## Getting a recommendation for a user based on user_id\n",
    "\n",
    "Upon testing for some sample users, it was noticeable that the models differ significantly in terms of recommendation.\n",
    "\n",
    "On a production system they could potentially be used in combination in order to retrieve candidates, adding variety, and then score those candidates with another model such as DNN.\n",
    "\n",
    "Note: For the final model, you would usually also retrain based on the full data, after the model is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ebb10ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 1 using BPR:\n",
      "   item_id                       movie_title\n",
      "0       98  Silence of the Lambs, The (1991)\n",
      "1      173        Princess Bride, The (1987)\n",
      "2      588       Beauty and the Beast (1991)\n",
      "3      433                   Heathers (1989)\n",
      "4      176                     Aliens (1986)\n",
      "Recommendations for user 1 using VAECF:\n",
      "   item_id                       movie_title\n",
      "0       98  Silence of the Lambs, The (1991)\n",
      "1      176                     Aliens (1986)\n",
      "2      234                       Jaws (1975)\n",
      "3      195            Terminator, The (1984)\n",
      "4      182                 GoodFellas (1990)\n"
     ]
    }
   ],
   "source": [
    "def get_recommendation(user_id, model):\n",
    "    #Function to get recommendations for a user\n",
    "    recommendations = model.recommend(user_id, train_set=rs.train_set, remove_seen=True, k=10)\n",
    "\n",
    "    df = pd.DataFrame([(item, items[items.item_id==item].movie_title.values[0]) for item in recommendations], columns=['item_id', 'movie_title'])\n",
    "\n",
    "    return df\n",
    "\n",
    "user_id=1\n",
    "# Get recommendation for each of the best model configurations\n",
    "for model in [gs_bpr.best_model, gs_vaecf.best_model]:\n",
    "    print(f\"Recommendations for user {user_id} using {model.name}:\")\n",
    "    print(get_recommendation(user_id=user_id, model=model).head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de8b15d",
   "metadata": {},
   "source": [
    "### Retrieving the scores as well\n",
    "\n",
    "This method is being used just for checking, it returns the items based on the internal encoding and it does not exclude already watched items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "ff66fc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  83   16  114 ... 1654 1655  274] [2.6922766e-03 2.1611936e-04 3.0348392e-04 ... 6.9536118e-06 9.7517805e-06\n",
      " 4.0015479e-06]\n"
     ]
    }
   ],
   "source": [
    "# Get the ranked items and their scores for the user, does not remove seen items, returned items are on item_idx, need to convert to item_id\n",
    "(ranked_items, item_scores) =  gs_vaecf.best_model.rank(user_idx=rs.train_set.uid_map[user_id], k=10)\n",
    "\n",
    "print(ranked_items, item_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2b5140",
   "metadata": {},
   "source": [
    "### Conversion example for reference\n",
    "\n",
    "The cornac library converts the original ids into what they call user_idx and item_idx, this can be a little confusing when looking at the prediction results, the follow dictionarys can be used to obtain the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "be21d182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converts user_id to user_idx\n",
    "rs.train_set.uid_map[943]\n",
    "\n",
    "# Converts item_id to item_idx\n",
    "rs.train_set.iid_map[568]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
